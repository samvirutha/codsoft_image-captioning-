# IMAGE CAPTIONING


!pip install transformers accelerate -q
!pip install git+https://github.com/huggingface/transformers.git -q  # latest version
!pip install git+https://github.com/huggingface/vision.git -q  # if not installed

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests
import torch

# Load model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load your image
image_path = "/content/sample_data_final/nature.jpg"
raw_image = Image.open(image_path).convert('RGB')

# Process the image
inputs = processor(raw_image, return_tensors="pt")

# Generate caption
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)

# Show image with caption
import matplotlib.pyplot as plt
plt.imshow(raw_image)
plt.axis("off")
plt.title(caption)
plt.show()
